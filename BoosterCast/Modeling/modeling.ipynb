{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4825d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_price(data):\n",
    "    \"\"\"\n",
    "    Cleans and converts the 'price' field from a string to a float.\n",
    "    Handles cases with missing values, commas, and unexpected formats.\n",
    "\n",
    "    :param data: Dictionary containing the 'price' field as a string.\n",
    "    :return: Updated dictionary with 'price' as a float.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        price_str = str(data.get(\"price\", \"\")).strip()\n",
    "        if not price_str:\n",
    "            raise ValueError(\"Empty price value\")\n",
    "\n",
    "        price_match = re.search(r\"[\\d,]+(\\.\\d{1,2})?\", price_str)\n",
    "        if not price_match:\n",
    "            raise ValueError(f\"Invalid price format: {price_str}\")\n",
    "\n",
    "        clean_price = float(price_match.group().replace(\",\", \"\"))\n",
    "        data[\"price\"] = clean_price\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"Warning: {e}. Setting price to None.\")\n",
    "        data[\"price\"] = None \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_condition(data):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes the 'condition' field by extracting key condition labels\n",
    "    and identifying special flags such as '4 Pack Minimum' and 'KOREAN'.\n",
    "\n",
    "    :param data: Dictionary containing the 'condition' field as a string\n",
    "    :return: Updated dictionary with standardized 'condition' and extracted flags\n",
    "    \"\"\"\n",
    "    condition_text = data.get(\"condition\", \"\").replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    condition_keywords = [\"Unopened\", \"Sealed\", \"Opened\", \"New\", \"Used\"]\n",
    "\n",
    "    primary_condition = None\n",
    "    for keyword in condition_keywords:\n",
    "        if keyword.lower() in condition_text.lower():\n",
    "            primary_condition = keyword\n",
    "            break\n",
    "\n",
    "    data[\"condition\"] = primary_condition if primary_condition else \"Unknown\"\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cf7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def clean_quantity(data, apply_log=False):\n",
    "    \"\"\"\n",
    "    Converts the 'quantity' field to an integer.\n",
    "    If conversion fails, sets the quantity to 0.\n",
    "    Optionally applies a log transform to the value.\n",
    "\n",
    "    :param data: Dictionary containing the 'quantity' field.\n",
    "    :param apply_log: Boolean to indicate whether to apply a logarithm transform.\n",
    "    :return: The updated dictionary with cleaned 'quantity'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        quantity = int(str(data.get(\"quantity\", \"\")).strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {e}. Setting quantity to 0.\")\n",
    "        quantity = 0\n",
    "\n",
    "    if apply_log and quantity > 0:\n",
    "        quantity = round(math.log(quantity), 4)\n",
    "\n",
    "    data[\"quantity\"] = quantity\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a00ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def clean_date(data):\n",
    "    \"\"\"\n",
    "    Converts the 'date' field from a string like \"2/15/25\" to a datetime object.\n",
    "    Also extracts additional features such as day_of_year and month.\n",
    "    \n",
    "    :param data: Dictionary containing the 'date' field.\n",
    "    :return: Updated dictionary with 'date' as a datetime object and extra date features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        date_str = data.get(\"date\", \"\").strip()\n",
    "        if not date_str:\n",
    "            raise ValueError(\"Empty date string\")\n",
    "        \n",
    "        dt = datetime.strptime(date_str, \"%m/%d/%y\")\n",
    "        data[\"date\"] = dt\n",
    "        \n",
    "        data[\"day_of_year\"] = dt.timetuple().tm_yday\n",
    "        data[\"month\"] = dt.month\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {e}. Date conversion failed for: {data.get('date')}\")\n",
    "        data[\"date\"] = None\n",
    "        data[\"day_of_year\"] = None\n",
    "        data[\"month\"] = None\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f947d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_process(market_history):\n",
    "    for item in market_history:\n",
    "        clean_price(item)\n",
    "        clean_condition(item)\n",
    "        clean_quantity(item)\n",
    "        clean_date(item)\n",
    "    return market_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aee4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load and prepare data\n",
    "def read_json_file(filepath):\n",
    "    \"\"\"Read JSON data from file\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "A = read_json_file('scraped_results.json')\n",
    "B = read_json_file('scraped_results2.json')\n",
    "\n",
    "for item in B:\n",
    "    A.append(item)\n",
    "for item in A:\n",
    "    cleaning_process(item)\n",
    "\n",
    "flattened_data = []\n",
    "for item_index, item_data in enumerate(A):\n",
    "    for record in item_data:\n",
    "        record[\"item_id\"] = item_index\n",
    "        flattened_data.append(record)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(flattened_data)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values('date')\n",
    "df['item_id'] = df['item_id'].astype('category').cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4984e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_temporal_features(df):\n",
    "    \"\"\"Add temporal features to the dataset.\"\"\"\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "    \n",
    "    # Fourier terms for seasonality\n",
    "    df['weekly_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['weekly_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['yearly_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "    df['yearly_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_lag_features(df):\n",
    "    \"\"\"Add lagged price features.\"\"\"\n",
    "    df['price_lag_1'] = df.groupby('item_id')['price'].shift(1)\n",
    "    df['price_lag_2'] = df.groupby('item_id')['price'].shift(2)\n",
    "    df['price_lag_7'] = df.groupby('item_id')['price'].shift(7)\n",
    "    return df\n",
    "\n",
    "def add_differenced_features(df):\n",
    "    \"\"\"Add features that capture price changes.\"\"\"\n",
    "    df['price_diff_1'] = df.groupby('item_id')['price'].diff(1)  # Daily change\n",
    "    df['price_diff_7'] = df.groupby('item_id')['price'].diff(7)   # Weekly change\n",
    "    df['price_pct_change_1'] = df.groupby('item_id')['price'].pct_change(1)\n",
    "    return df\n",
    "\n",
    "def add_rolling_features(df):\n",
    "    \"\"\"Add rolling statistics.\"\"\"\n",
    "    windows = [3, 7, 14, 30]  # Multiple window sizes\n",
    "    for window in windows:\n",
    "        df[f'rolling_avg_{window}'] = df.groupby('item_id')['price'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        df[f'rolling_std_{window}'] = df.groupby('item_id')['price'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def add_mean_reversion_features(df):\n",
    "    \"\"\"Add features that capture deviation from typical levels.\"\"\"\n",
    "    # Deviation from rolling averages\n",
    "    df['deviation_from_7day'] = df['price'] - df['rolling_avg_7']\n",
    "    df['deviation_from_30day'] = df['price'] - df['rolling_avg_30']\n",
    "    \n",
    "    # Z-score of current price\n",
    "    df['price_zscore'] = df.groupby('item_id')['price'].transform(\n",
    "        lambda x: (x - x.rolling(window=30, min_periods=1).mean()) / \n",
    "                 x.rolling(window=30, min_periods=1).std()\n",
    "    )\n",
    "    \n",
    "    # Price position within recent range\n",
    "    df['price_percentile_30'] = df.groupby('item_id')['price'].transform(\n",
    "        lambda x: x.rolling(window=30, min_periods=1).apply(\n",
    "            lambda y: np.sum(y < y.iloc[-1]) / len(y) if len(y) > 1 else 0.5\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Apply all feature engineering\n",
    "df = add_temporal_features(df)\n",
    "df = add_lag_features(df)\n",
    "df = add_differenced_features(df)\n",
    "df = add_rolling_features(df)\n",
    "df = add_mean_reversion_features(df)\n",
    "\n",
    "# Drop rows with missing values (from lag features)\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ecdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "# Define feature columns\n",
    "feature_columns = [\n",
    "    'quantity',\n",
    "    'day_of_year', 'month', 'day_of_week', 'week_of_year',\n",
    "    'weekly_sin', 'weekly_cos', 'yearly_sin', 'yearly_cos',\n",
    "    'price_lag_1', 'price_lag_2', 'price_lag_7',\n",
    "    'price_diff_1', 'price_diff_7', 'price_pct_change_1',\n",
    "    'rolling_avg_3', 'rolling_avg_7', 'rolling_avg_14', 'rolling_avg_30',\n",
    "    'rolling_std_7', 'rolling_std_30',\n",
    "    'deviation_from_7day', 'deviation_from_30day',\n",
    "    'price_zscore', 'price_percentile_30'\n",
    "]\n",
    "\n",
    "# Define target variable\n",
    "target_column = 'price'\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[feature_columns]\n",
    "y = df[target_column]\n",
    "\n",
    "# Time-Based Validation with prediction adjustment\n",
    "def time_based_validation(X, y):\n",
    "    \"\"\"Perform time-based validation using walk-forward splits.\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    results = []\n",
    "    last_test_index = None\n",
    "    models = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train Random Forest model\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        models.append(model)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Apply mean reversion adjustment\n",
    "        deviation = X_test['deviation_from_7day'].values\n",
    "        adjustment_factor = np.where(\n",
    "            deviation > 0,\n",
    "            1 - np.tanh(deviation / X_test['rolling_std_7'].clip(lower=0.1)),\n",
    "            1 + np.tanh(-deviation / X_test['rolling_std_7'].clip(lower=0.1))\n",
    "        )\n",
    "        y_pred_adjusted = y_pred * adjustment_factor\n",
    "\n",
    "        # Evaluate both raw and adjusted predictions\n",
    "        for pred, label in [(y_pred, 'raw'), (y_pred_adjusted, 'adjusted')]:\n",
    "            mae = mean_absolute_error(y_test, pred)\n",
    "            mse = mean_squared_error(y_test, pred)\n",
    "            r2 = r2_score(y_test, pred)\n",
    "            \n",
    "            results.append({\n",
    "                'fold': len(results)//2 + 1,\n",
    "                'type': label,\n",
    "                'mae': mae,\n",
    "                'mse': mse,\n",
    "                'r2': r2,\n",
    "                'up_percentage': np.mean(np.diff(pred) > 0)  # Track upward movements\n",
    "            })\n",
    "\n",
    "        last_test_index = test_index\n",
    "\n",
    "    return results, models[-1], last_test_index\n",
    "\n",
    "# Run time-based validation\n",
    "validation_results, model, last_test_index = time_based_validation(X, y)\n",
    "\n",
    "# Print validation results\n",
    "print(\"\\nValidation Results:\")\n",
    "for i in range(0, len(validation_results), 2):\n",
    "    raw = validation_results[i]\n",
    "    adj = validation_results[i+1]\n",
    "    print(f\"\\nFold {raw['fold']}:\")\n",
    "    print(f\"  Raw:      MAE = {raw['mae']:.2f}, MSE = {raw['mse']:.2f}, R² = {raw['r2']:.4f}, Up% = {raw['up_percentage']:.2f}\")\n",
    "    print(f\"  Adjusted: MAE = {adj['mae']:.2f}, MSE = {adj['mse']:.2f}, R² = {adj['r2']:.4f}, Up% = {adj['up_percentage']:.2f}\")\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, \"pokemon_forecaster_v3.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
